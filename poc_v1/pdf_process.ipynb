{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7b6639",
   "metadata": {},
   "source": [
    "## Imports and notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774cd860",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb112b9c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain-huggingface langchain-community pypdf langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f87e4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install chromadb smolagents python-dotenv gradio sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9007b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44739108",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade langchain langchain-community langchain-huggingface chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f148704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.getcwd())\n",
    "# base_dir = input(\"Enter the base directory path for the dataset: \")\n",
    "\n",
    "# # Verify the path\n",
    "# print(\"Base directory set to:\", base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66e45e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\sindh\\AppData\\Local\\Temp\\ipykernel_31976\\238155888.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  base_dir = \"..\\data\\Legal-Tactics-Book.zip\"\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"..\\data\\Legal-Tactics-Book.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4295c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDFs into chunks...\n",
      "Generated 2934 document chunks.\n",
      "Building the vector store...\n",
      "Removing existing vector store from c:/chroma_db\n",
      "Total chunks received for vector store: 2934\n",
      "Example chunk: ## Chapter 17: Condominium Control ▲\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sindh\\AppData\\Local\\Temp\\ipykernel_31976\\2451163269.py:117: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding_model = OpenAIEmbeddings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and saving the new vector store with OpenAI embeddings...\n",
      "Vector store successfully created at c:/chroma_db\n",
      "\n",
      "Top Matching Results:\n",
      "1. Legal Tactics: Tenants Rights in\n",
      "Massachusetts May 2017\n",
      "372 ▲ Chapter 16: Mobile Homes...\n",
      "2. Law.\" This notice must inform you of your\n",
      "Mobile home park tenants in Massachusetts have rights and be in the exact language\n",
      "a number of very important rights before contained in the law.\n",
      "moving into a mobile home park. A park owner\n",
      "cannot refuse to rent a lot to you if you meet the The park owner m...\n",
      "3. may become a tenant at will.12\n",
      "Tenancy by Regulation\n",
      "If you are a tenant in a mobile home or public or subsidized housing, you are a\n",
      "tenant by regulation.13 You may have more protections as a tenant.\n",
      "If you live in a mobile home, see Chapter 16: Mobile Homes.\n",
      "If you live in public or subsidized hous...\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pdfplumber\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import shutil\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "# Get the API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai_api_key is None:\n",
    "    print(\"Error: OPENAI_API_KEY not found in .env file.\")\n",
    "    exit()\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "def extract_zip(uploaded_zip_path, extract_to=\"../temp_pdfs\"):\n",
    "    \"\"\"Extracts a zipped folder containing PDFs.\"\"\"\n",
    "    os.makedirs(extract_to, exist_ok=True)  # Ensure extraction folder exists\n",
    "\n",
    "    with zipfile.ZipFile(uploaded_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    pdf_files = []\n",
    "    for root, _, files in os.walk(extract_to):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".pdf\"):\n",
    "                pdf_files.append(os.path.join(root, file))\n",
    "\n",
    "    return pdf_files\n",
    "\n",
    "def pdf_to_markdown_string(pdf_path):\n",
    "    \"\"\"Extracts structured content while preserving section headers.\"\"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        sections = {}\n",
    "        current_section = None\n",
    "        skipped_pages = 2  # Skip first two pages bc table of contents extends \n",
    "\n",
    "        for page_num, page in enumerate(pdf.pages[skipped_pages:]):  \n",
    "            text = page.extract_text()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            lines = text.split(\"\\n\")\n",
    "\n",
    "            for line in lines:\n",
    "                if re.match(r\"^\\s*Chapter \\d+:?\", line) or re.match(r\"^[A-Z][A-Z\\s]+$\", line.strip()):\n",
    "                    current_section = line.strip()\n",
    "                    sections[current_section] = sections.get(current_section, \"\")\n",
    "                elif current_section:\n",
    "                    sections[current_section] += line.strip() + \"\\n\"\n",
    "\n",
    "    # Ensure section headers stay attached to their respective text chunks\n",
    "    markdown_chunks = [f\"## {section}\\n\\n{content.strip()}\\n\\n\" for section, content in sections.items()]\n",
    "    return markdown_chunks\n",
    "\n",
    "def determine_role(text):\n",
    "    \"\"\"Assigns a role based on detected keywords. Might need to do a bit more research \n",
    "    on keywords themselves to see which ones are the correct keywords to put in each list.\"\"\"\n",
    "    tenant_keywords = [\"tenant rights\", \"rent control\", \"eviction protections\", \"lease termination\"]\n",
    "    landlord_keywords = [\"landlord duties\", \"property maintenance\", \"rent collection\", \"eviction process\"]\n",
    "\n",
    "    if any(keyword in text.lower() for keyword in tenant_keywords):\n",
    "        return \"tenant\"\n",
    "    elif any(keyword in text.lower() for keyword in landlord_keywords):\n",
    "        return \"landlord\"\n",
    "    return \"general\"  # Default if no clear role is identified\n",
    "\n",
    "def split_text_with_headers(text_splitter, markdown_sections):\n",
    "    \"\"\"Splits text while ensuring section headers remain in context.\"\"\"\n",
    "    all_chunks = []\n",
    "\n",
    "    for section in markdown_sections:\n",
    "        section_title = section.split(\"\\n\")[0]  # Extract the first line (header)\n",
    "        chunks = text_splitter.split_text(section)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            enriched_chunk = f\"{section_title}\\n\\n{chunk}\"  # Attach section header to each chunk\n",
    "            all_chunks.append(enriched_chunk)\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "def load_and_process_pdfs(zip_path):\n",
    "    \"\"\"Processes a zipped folder of PDFs, preserves headers, and splits into smaller chunks for vector storage.\"\"\"\n",
    "    pdf_files = extract_zip(zip_path)\n",
    "    all_chunks = []\n",
    "    \n",
    "    # Use a text splitter to break sections into smaller chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # Adjust chunk size as needed?? Will need to experiment with this and how it interacts with the prompts we try out too - made bigger from 500\n",
    "        chunk_overlap=200 # Made overlapping size bigger from 50\n",
    "    )\n",
    "\n",
    "    for pdf in pdf_files:\n",
    "        markdown_sections = pdf_to_markdown_string(pdf)\n",
    "\n",
    "        # Use new method to ensure headers remain attached to their respective text chunks\n",
    "        enriched_chunks = split_text_with_headers(text_splitter, markdown_sections)\n",
    "\n",
    "        for chunk in enriched_chunks:\n",
    "            role = determine_role(chunk)  # Assign role based on text analysis\n",
    "            all_chunks.append(Document(page_content=chunk, metadata={\"source\": os.path.basename(pdf), \"role\": role}))\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "def create_vector_store(chunks, persist_dir: str):\n",
    "    \"\"\"Create and persist a Chroma vector store using OpenAI embeddings.\"\"\"\n",
    "    \n",
    "    if os.path.exists(persist_dir):\n",
    "        print(f\"Removing existing vector store from {persist_dir}\")\n",
    "        shutil.rmtree(persist_dir)  # Try commenting this out if issues persist\n",
    "\n",
    "    # Debugging info\n",
    "    print(f\"Total chunks received for vector store: {len(chunks)}\")\n",
    "    if chunks:\n",
    "        print(f\"Example chunk: {chunks[0].page_content[:300]}\")\n",
    "\n",
    "    try:\n",
    "        # Initialize OpenAI Embeddings\n",
    "        embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "        print(\"Building and saving the new vector store with OpenAI embeddings...\")\n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=persist_dir\n",
    "        )\n",
    "        return vector_db\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {e}\")\n",
    "        return None  # Return None if an error occurs\n",
    "\n",
    "\n",
    "def query_vector_store(vector_db, query, role=\"general\"):\n",
    "    \"\"\"Finds the most relevant chunks based on the query.\"\"\"\n",
    "    \"\"\"Switching from similarity to MMR bc MMR prioritizes diversity in the results, \n",
    "    ensuring a mix of relevant but non-redundant information, \n",
    "    whereas similarity search focuses solely on the closest matches.\"\"\"\n",
    "     # results = vector_db.similarity_search(query, k=3)  # Retrieve top most relevant results\n",
    "    results = vector_db.max_marginal_relevance_search(query, k=5) # Might need to play around with K value here\n",
    "    # Filter results based on role metadata\n",
    "    filtered_results = [doc.page_content for doc in results if doc.metadata.get(\"role\", \"general\") == role]\n",
    "    \n",
    "    if not filtered_results:  # Fallback if no perfect match is found\n",
    "        filtered_results = [doc.page_content for doc in results]\n",
    "\n",
    "    return filtered_results[:3]  # Return top 3 refined results\n",
    "\n",
    "def main():\n",
    "    zip_file_path = base_dir \n",
    "    vector_db_dir = os.path.join(os.getcwd(), \"/chroma_db\") # Added slash\n",
    "\n",
    "    print(\"Processing PDFs into chunks...\")\n",
    "    document_chunks = load_and_process_pdfs(\"your_zip_path.zip\")\n",
    "    print(f\"Total chunks created: {len(document_chunks)}\")\n",
    "    # Tests\n",
    "    print(f\"Example chunk:\\n{document_chunks[0].page_content[:500]}\")\n",
    "    print(f\"Metadata: {document_chunks[0].metadata}\")\n",
    "\n",
    "    print(\"Building the vector store...\")\n",
    "    vector_db = create_vector_store(document_chunks, vector_db_dir)\n",
    "    print(f\"Vector store successfully created at {vector_db_dir}\")\n",
    "\n",
    "    # Example queries\n",
    "    tenant_query = \"What rights do tenants have during eviction?\"\n",
    "    landlord_query = \"What obligations do landlords have for maintenance?\"\n",
    "\n",
    "    tenant_results = query_vector_store(vector_db, tenant_query, role=\"tenant\")\n",
    "    landlord_results = query_vector_store(vector_db, landlord_query, role=\"landlord\")\n",
    "\n",
    "    print(\"\\nTenant Response:\")\n",
    "    for result in tenant_results:\n",
    "        print(result[:300])\n",
    "\n",
    "    print(\"\\nLandlord Response:\")\n",
    "    for result in landlord_results:\n",
    "        print(result[:300])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlite3\n",
    "# # 'c:/chroma_db/chroma.sqlite3'\n",
    "\n",
    "\n",
    "# def read_chroma_db(db_file=\"c:/chroma_db/chroma.sqlite3\"): #Default filename\n",
    "#     try:\n",
    "#         conn = sqlite3.connect(db_file)\n",
    "#         cursor = conn.cursor()\n",
    "\n",
    "#         # Example: Get table names\n",
    "#         cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "#         tables = cursor.fetchall()\n",
    "\n",
    "#         print(\"Tables in chroma.sqlite3:\")\n",
    "#         for table in tables:\n",
    "#             print(table[0])\n",
    "\n",
    "#         # Example: Read data from the first table (if any)\n",
    "#         if tables:\n",
    "#             first_table = tables[0][0]  # Get the name of the first table\n",
    "#             cursor.execute(f\"SELECT * FROM {first_table};\")\n",
    "#             rows = cursor.fetchall()\n",
    "\n",
    "#             print(f\"\\nData from {first_table}:\")\n",
    "#             for row in rows:\n",
    "#                 print(row)\n",
    "\n",
    "#     except sqlite3.Error as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "\n",
    "#     finally:\n",
    "#         if conn:\n",
    "#             conn.close()\n",
    "\n",
    "# read_chroma_db() #uses the default filename."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
