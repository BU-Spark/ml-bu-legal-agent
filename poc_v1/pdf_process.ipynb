{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7b6639",
   "metadata": {},
   "source": [
    "## Imports and notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pdfplumber\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import shutil\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import torch\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e45e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"..\\data\\Legal-Tactics-Book.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f04771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "# Get the API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai_api_key is None:\n",
    "    print(\"Error: OPENAI_API_KEY not found in .env file.\")\n",
    "    exit()\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4295c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip(uploaded_zip_path, extract_to=\"../temp_pdfs\"):\n",
    "    \"\"\"Extracts a zipped folder containing PDFs.\"\"\"\n",
    "    os.makedirs(extract_to, exist_ok=True)  # Ensure extraction folder exists\n",
    "\n",
    "    with zipfile.ZipFile(uploaded_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    pdf_files = []\n",
    "    for root, _, files in os.walk(extract_to):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".pdf\"):\n",
    "                pdf_files.append(os.path.join(root, file))\n",
    "\n",
    "    return pdf_files\n",
    "\n",
    "def pdf_to_markdown_string(pdf_path):\n",
    "    \"\"\"Extracts structured content while preserving section headers.\"\"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        sections = {}\n",
    "        current_section = None\n",
    "        skipped_pages = 2  # Skip first two pages bc table of contents extends \n",
    "\n",
    "        for page_num, page in enumerate(pdf.pages[skipped_pages:]):  \n",
    "            text = page.extract_text()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            lines = text.split(\"\\n\")\n",
    "\n",
    "            for line in lines:\n",
    "                if re.match(r\"^\\s*Chapter \\d+:?\", line) or re.match(r\"^[A-Z][A-Z\\s]+$\", line.strip()):\n",
    "                    current_section = line.strip()\n",
    "                    sections[current_section] = sections.get(current_section, \"\")\n",
    "                elif current_section:\n",
    "                    sections[current_section] += line.strip() + \"\\n\"\n",
    "\n",
    "    # Ensure section headers stay attached to their respective text chunks\n",
    "    markdown_chunks = [f\"## {section}\\n\\n{content.strip()}\\n\\n\" for section, content in sections.items()]\n",
    "    return markdown_chunks\n",
    "\n",
    "def determine_role(text):\n",
    "    \"\"\"Assigns a role based on detected keywords. Might need to do a bit more research \n",
    "    on keywords themselves to see which ones are the correct keywords to put in each list.\"\"\"\n",
    "    tenant_keywords = [\"tenant rights\", \"rent control\", \"eviction protections\", \"lease termination\"]\n",
    "    landlord_keywords = [\"landlord duties\", \"property maintenance\", \"rent collection\", \"eviction process\"]\n",
    "\n",
    "    if any(keyword in text.lower() for keyword in tenant_keywords):\n",
    "        return \"tenant\"\n",
    "    elif any(keyword in text.lower() for keyword in landlord_keywords):\n",
    "        return \"landlord\"\n",
    "    return \"general\"  # Default if no clear role is identified\n",
    "\n",
    "def split_text_with_headers(text_splitter, markdown_sections):\n",
    "    \"\"\"Splits text while ensuring section headers remain in context.\"\"\"\n",
    "    all_chunks = []\n",
    "\n",
    "    for section in markdown_sections:\n",
    "        section_title = section.split(\"\\n\")[0]  # Extract the first line (header)\n",
    "        chunks = text_splitter.split_text(section)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            enriched_chunk = f\"{section_title}\\n\\n{chunk}\"  # Attach section header to each chunk\n",
    "            all_chunks.append(enriched_chunk)\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "def load_and_process_pdfs(zip_path):\n",
    "    \"\"\"Processes a zipped folder of PDFs, preserves headers, and splits into smaller chunks for vector storage.\"\"\"\n",
    "    pdf_files = extract_zip(zip_path)\n",
    "    all_chunks = []\n",
    "    \n",
    "    # Use a text splitter to break sections into smaller chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # Adjust chunk size as needed?? Will need to experiment with this and how it interacts with the prompts we try out too - made bigger from 500\n",
    "        chunk_overlap=200 # Made overlapping size bigger from 50\n",
    "    )\n",
    "\n",
    "    for pdf in pdf_files:\n",
    "        markdown_sections = pdf_to_markdown_string(pdf)\n",
    "\n",
    "        # Use new method to ensure headers remain attached to their respective text chunks\n",
    "        enriched_chunks = split_text_with_headers(text_splitter, markdown_sections)\n",
    "\n",
    "        for chunk in enriched_chunks:\n",
    "            role = determine_role(chunk)  # Assign role based on text analysis\n",
    "            all_chunks.append(Document(page_content=chunk, metadata={\"source\": os.path.basename(pdf), \"role\": role}))\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "def create_vector_store(chunks, persist_dir: str):\n",
    "    \"\"\"Create and persist a Chroma vector store using OpenAI embeddings.\"\"\"\n",
    "    \n",
    "    if os.path.exists(persist_dir):\n",
    "        print(f\"Removing existing vector store from {persist_dir}\")\n",
    "        shutil.rmtree(persist_dir)  # Try commenting this out if issues persist\n",
    "\n",
    "    # Debugging info\n",
    "    print(f\"Total chunks received for vector store: {len(chunks)}\")\n",
    "    if chunks:\n",
    "        print(f\"Example chunk: {chunks[0].page_content[:300]}\")\n",
    "\n",
    "    try:\n",
    "        # Initialize OpenAI Embeddings\n",
    "        embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "        print(\"Building and saving the new vector store with OpenAI embeddings...\")\n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=persist_dir\n",
    "        )\n",
    "        return vector_db\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {e}\")\n",
    "        return None  # Return None if an error occurs\n",
    "\n",
    "\n",
    "def query_vector_store(vector_db, query, role=\"general\"):\n",
    "    \"\"\"Finds the most relevant chunks based on the query.\"\"\"\n",
    "    \"\"\"Switching from similarity to MMR bc MMR prioritizes diversity in the results, \n",
    "    ensuring a mix of relevant but non-redundant information, \n",
    "    whereas similarity search focuses solely on the closest matches.\"\"\"\n",
    "     # results = vector_db.similarity_search(query, k=3)  # Retrieve top most relevant results\n",
    "    results = vector_db.max_marginal_relevance_search(query, k=5) # Might need to play around with K value here\n",
    "    # Filter results based on role metadata\n",
    "    filtered_results = [doc.page_content for doc in results if doc.metadata.get(\"role\", \"general\") == role]\n",
    "    \n",
    "    if not filtered_results:  # Fallback if no perfect match is found\n",
    "        filtered_results = [doc.page_content for doc in results]\n",
    "\n",
    "    return filtered_results[:3]  # Return top 3 refined results\n",
    "\n",
    "def main():\n",
    "    zip_file_path = base_dir \n",
    "    vector_db_dir = os.path.join(os.getcwd(), \"/chroma_db\") # Added slash\n",
    "\n",
    "    print(\"Processing PDFs into chunks...\")\n",
    "    document_chunks = load_and_process_pdfs(\"your_zip_path.zip\")\n",
    "    print(f\"Total chunks created: {len(document_chunks)}\")\n",
    "    # Tests\n",
    "    print(f\"Example chunk:\\n{document_chunks[0].page_content[:500]}\")\n",
    "    print(f\"Metadata: {document_chunks[0].metadata}\")\n",
    "\n",
    "    print(\"Building the vector store...\")\n",
    "    vector_db = create_vector_store(document_chunks, vector_db_dir)\n",
    "    print(f\"Vector store successfully created at {vector_db_dir}\")\n",
    "\n",
    "    # Example queries\n",
    "    tenant_query = \"What rights do tenants have during eviction?\"\n",
    "    landlord_query = \"What obligations do landlords have for maintenance?\"\n",
    "\n",
    "    tenant_results = query_vector_store(vector_db, tenant_query, role=\"tenant\")\n",
    "    landlord_results = query_vector_store(vector_db, landlord_query, role=\"landlord\")\n",
    "\n",
    "    print(\"\\nTenant Response:\")\n",
    "    for result in tenant_results:\n",
    "        print(result[:300])\n",
    "\n",
    "    print(\"\\nLandlord Response:\")\n",
    "    for result in landlord_results:\n",
    "        print(result[:300])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
